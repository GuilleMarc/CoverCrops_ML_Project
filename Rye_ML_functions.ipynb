{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries needed:\n",
    "import warnings\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.preprocessing import quantile_transform\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn import tree\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, learning_curve\n",
    "import pickle                                                                      # to save trained models to disk\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------- Data preparation  ----------------------------------------------\n",
    "# Functions to normalize the highly-correlated feature matrix using quantile_reg  --- FUNCTION 1\n",
    "def X_transform(x):\n",
    "    warnings.filterwarnings(action=\"ignore\")\n",
    "    x_np= np.array(x).reshape((len(x),1))\n",
    "    return quantile_transform(x_np, \n",
    "                              output_distribution='normal',\n",
    "                              random_state=0).squeeze()\n",
    "\n",
    "def X_data_transf(data,features):\n",
    "    warnings.filterwarnings(action=\"ignore\")\n",
    "    data_mat= np.matrix(data.loc[:,features].apply(X_transform,axis=0))\n",
    "    return np.array(data_mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------- Model training and testing  ----------------------------------------------\n",
    "# This is material for appendices or supp. materials in the manuscript\n",
    "\n",
    "## --------------------------------- Pipeline for easier model fitting: ----------------------------------------\n",
    "# Functions to compute prediction errors plus Bias and variance for each model         # --- FUNCTION 2\n",
    "\n",
    "def error_functions(obs,pred):                                                           \n",
    "    '''\n",
    "    Compute Model BIAS and model Variance\n",
    "    Complementary to MSE, MAE, R2 (agreement)\n",
    "    Model BIAS and model Variance (important to assess precision/accuracy of each model)\n",
    "    Usually, MSE= bias^2 + var + error; with MSE~var if estimator is unbiased\n",
    "    '''\n",
    "    bias= np.mean( (obs - pred) )\n",
    "    var= np.mean( (pred - np.mean(pred))**2 )\n",
    "    rmse= np.sqrt(mean_squared_error(obs,pred))\n",
    "    mae= mean_absolute_error(obs,pred)\n",
    "    r2= r2_score(obs,pred)\n",
    "    \n",
    "    return pd.DataFrame({\"bias\": [bias],\"var\": [var], \"rmse\": [rmse],'mae': [mae], 'r2': [r2]})\n",
    "\n",
    "# Function to evaluate model performance and choose candidates                        #--- FUNCTION 3\n",
    "\n",
    "def model_train_test (output, x_sample, test_size=0.70, phase=\"train\", model= LinearRegression()): \n",
    "    '''\n",
    "    Return model performance for a candidate ML model\n",
    "    TODO:fill in more explanations\n",
    "    '''\n",
    "    # model set-up\n",
    "    model= model\n",
    "    # Sample sizes for model train/test:\n",
    "    X_train, X_test, y_train, y_test= train_test_split(x_sample, output, test_size=test_size, random_state=42)\n",
    "    # fit and predict\n",
    "    model.fit(X_train,y_train)\n",
    "    if phase == \"train\":\n",
    "        pred= model.predict(X_train)\n",
    "        err= error_functions(y_train,pred)\n",
    "    else:\n",
    "        pred= model.predict(X_test)\n",
    "        err= error_functions(y_test,pred)\n",
    "    return err"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------- Model Validation  ----------------------------------------------\n",
    "# This is material for appendices or supp. materials in the manuscript\n",
    "# ---------------------- Function to implement a Leave-one-out Cross Validation ----------------------\n",
    "def model_validation(y_sample, x_sample, model=LinearRegression()):         # ---- FUNCTION 4\n",
    "    '''\n",
    "    This function implements Leave-one-out cross validation \n",
    "    It iterates over each observation, fitting the model to\n",
    "    all but the observation of interest. Then, predicts\n",
    "    and computes errors for that observation.\n",
    "    LOOCV_error is the mean error estimate over the n-set of preds.\n",
    "    '''\n",
    "    \n",
    "    warnings.filterwarnings(action=\"ignore\")\n",
    "    model= model\n",
    "    output_length= x_sample.shape[0]\n",
    "    \n",
    "    #Create objects to store predictions and errors\n",
    "    pred= np.empty(0)\n",
    "    pred_error= np.empty(0)\n",
    "    pred_sq_error= np.empty(0)\n",
    "    \n",
    "    # Begin the iterations: train on [n-i] and predict for [i]\n",
    "    for i in range(output_length):\n",
    "        \n",
    "        #Prepare train and testing sets\n",
    "        x_train= np.vstack(( x_sample[:i], x_sample[i+1:]))\n",
    "        y_train= np.append(y_sample[:i], y_sample[i+1:]).reshape((output_length-1,1))\n",
    "        \n",
    "        #train in all but the i-th iteration\n",
    "        model_i= model.fit(x_train, y_train)\n",
    "        \n",
    "        #predict for the i-th iteration\n",
    "        pred_i= model_i.predict(x_sample[i].reshape(1,-1))\n",
    "        \n",
    "        #error for the i-th iteration (To compute BIAS)\n",
    "        pred_error_i= (y_sample[i].reshape(1,-1) - pred_i)\n",
    "        \n",
    "        #square error for the ith iteration (To compute MSE)\n",
    "        pred_sq_error_i= pred_error_i**2\n",
    "        \n",
    "        # Store error metrics\n",
    "        pred= np.append(pred, pred_i)\n",
    "        pred_error= np.append(pred_error, pred_error_i)\n",
    "        pred_sq_error= np.append(pred_sq_error, pred_sq_error_i)\n",
    "    \n",
    "    # Compute LOOCV results\n",
    "    mean_pred= np.mean(pred)\n",
    "    estim_sq_error= (pred - mean_pred)**2\n",
    "    r2= r2_score(y_true=y_sample, y_pred=pred)\n",
    "    \n",
    "    # Store all together\n",
    "    model_metrics={\"pred\": pred,\n",
    "                   \"bias\": pred_error,              \n",
    "                   \"var\": estim_sq_error,\n",
    "                   \"mse\": pred_sq_error,\n",
    "                   \"mae\": np.abs(pred_error)}           \n",
    "    print(\"Mean prediction estimate\", mean_pred)\n",
    "    print(\"Model R2: {}\".format( round(r2, 3)))\n",
    "    return pd.DataFrame(model_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------- Model \"Application\"  ----------------------------------------------\n",
    "# This is material for appendices or supp. materials in the manuscript\n",
    "# -------- Functions to select, filter, and extract predictions at diff. N environments -----------------\n",
    "# -------- Functions to resample and generate m-bootstrap estimates of 95% CIs (Biomass,N-shoot content)\n",
    "# \n",
    "\n",
    "# Select location and n-rate combination\n",
    "def select_data(location, year, n_rate_fall, n_rate_spring, data):\n",
    "    y= data[\n",
    "        (data[\"location\"] == location) &\n",
    "        (data[\"year\"] == year) &\n",
    "        (data[\"n_rate_fall\"] == n_rate_fall) &\n",
    "        (data[\"n_rate_spring\"] == n_rate_spring)\n",
    "    ]\n",
    "    return y\n",
    "\n",
    "#Generate m-samples out of a 4-element array(i.e. 3 our of 4 plots per N-rate treatment)\n",
    "def y_boots(values, sample_size, n_samples):\n",
    "    np.random.seed(0)\n",
    "    indices= np.random.randint(0,4,(n_samples, sample_size))\n",
    "    samples= np.empty(0)\n",
    "    for index in np.arange(indices.shape[0]):\n",
    "        sample_i= values[indices[index]]\n",
    "        samples= np.append(samples, sample_i)\n",
    "    return samples.reshape(n_samples, sample_size)\n",
    "\n",
    "# Generate n-bootstrap estimates of mean biomass, and 95 CIs at every N-rate combination (Per location)\n",
    "def bootstraps_summary(n_samples, sample_size, data, output):\n",
    "    # Resample biomass\n",
    "    y_bst= y_boots(values= data[output].values,\n",
    "                   sample_size= sample_size,\n",
    "                   n_samples= n_samples)\n",
    "    \n",
    "    # Mean, sd, and error bars per sample\n",
    "    y_bst_means= np.apply_along_axis(np.mean, 1, y_bst)\n",
    "    y_bst_sd= np.apply_along_axis(np.std, 1, y_bst) # std per sample\n",
    "    y_bst_err= 1.96 * (y_bst_sd/np.sqrt(sample_size))                          \n",
    "    \n",
    "    y_bootstrap_dict= {\n",
    "        \"mean_bst\": round(y_bst_means.mean(), 3),\n",
    "        \"std_bst\": round(y_bst_sd.mean(), 3),\n",
    "        \"error_bst\": round(y_bst_err.mean(),3)\n",
    "    }\n",
    "    \n",
    "    y_bootstrap_df= pd.DataFrame(y_bootstrap_dict, index= np.arange(1))\n",
    "    # Select row information from data to attach to each bootstrap estimate\n",
    "    treatment_info= data.iloc[:1,[0,1,2,4,5]]\n",
    "    # Put everything together\n",
    "    y_bst_df= pd.concat([\n",
    "        treatment_info.reset_index(drop=True),\n",
    "        y_bootstrap_df\n",
    "    ],\n",
    "    axis=1)\n",
    "    return y_bst_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------- optional functions ----------------------------------------------\n",
    "# This is material for appendices or supp. materials in the manuscript\n",
    "# -------- Functions to detect features of importance: Random Forest ----------------------------------\n",
    "\n",
    "def feat_importance(X,y):\n",
    "    '''\n",
    "    TODO: best_model should be an instatiation of the to-be-optimized model\n",
    "    x_original,cols\n",
    "    '''\n",
    "    best_model= RandomForestRegressor()  ## add in the optimized parameters\n",
    "    best_model.fit(X,y)\n",
    "    \n",
    "    importances= best_model.feature_importances_\n",
    "    std= np.std([tree.feature_importances_ for tree in best_model.estimators_],axis=0)\n",
    "    indices= np.argsort(importances)[::-1]\n",
    "    \n",
    "    # Plot \n",
    "    plt.figure()\n",
    "    plt.title(\"Feature importance Rye-biomass model, Random Forest\")\n",
    "    plt.bar(range(X.shape[1]), importances[indices],\n",
    "           color='r', yerr=std[indices], align=\"center\")\n",
    "    plt.xticks(range(X.shape[1]), indices)\n",
    "    plt.xlim([-1, X.shape[1]])\n",
    "    plt.show()\n",
    "    \n",
    "    #print(x_original[cols].columns[indices])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
